{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Neural Network Model for Regression\n",
    "\n",
    "This notebook demonstrates the implementation of a neural network model for regression tasks using the California Housing dataset, the Energy Efficiency dataset, and the Concrete Compressive Strength dataset. The model includes feature selection using correlation and Variance Inflation Factor (VIF) analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this tutorial, we will build a neural network model for regression tasks on tabular data. We will cover the following steps:\n",
    "\n",
    "1. **Data Loading:** Loading different datasets for regression tasks.\n",
    "2. **Feature Selection:** Using correlation and Variance Inflation Factor (VIF) to select important features.\n",
    "3. **Data Preprocessing:** Scaling and splitting the data into training and testing sets.\n",
    "4. **Model Building:** Creating a neural network model with fully connected layers and dropout for regularization.\n",
    "5. **Training the Model:** Training the model and using early stopping to prevent overfitting.\n",
    "6. **Evaluating the Model:** Evaluating the model using Mean Squared Error (MSE), Mean Absolute Error (MAE), and R-squared score (R2).\n",
    "7. **Visualization:** Visualizing the training loss and the predictions vs actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries\n",
    "\n",
    "First, we need to import the necessary libraries for data manipulation, visualization, and building the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Neural Network Model\n",
    "\n",
    "We define a neural network model class that handles data preprocessing, feature selection using correlation and VIF, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularNNModel:\n",
    "    def __init__(self, \n",
    "                 hidden_dims=[128, 64, 32], \n",
    "                 lr=0.0001, \n",
    "                 batch_size=32, \n",
    "                 epochs=5, \n",
    "                 dropout_rate=0.0\n",
    "                 ):\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.train_losses = []\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.lr = lr\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def feature_selection(self, X, y):\n",
    "        df = X.copy()\n",
    "        df['target'] = y\n",
    "        \n",
    "        correlation_matrix = df.corr()\n",
    "        \n",
    "        # Plotting the correlation matrix\n",
    "        plt.figure(figsize=(12,8))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "        plt.title('Correlation Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "        # Display correlations with the target\n",
    "        print(\"Correlation with target:\")\n",
    "        target_correlation = correlation_matrix['target'].sort_values(ascending=False)\n",
    "        print(target_correlation)\n",
    "        \n",
    "        # Drop features with low correlation with the target (threshold can be adjusted)\n",
    "        threshold = 0.1\n",
    "        selected_features = target_correlation[abs(target_correlation) > threshold].index.tolist()\n",
    "        selected_features.remove('target')\n",
    "        \n",
    "        # Calculate VIF to check multicollinearity\n",
    "        X_selected = df[selected_features]\n",
    "        vif_data = pd.DataFrame()\n",
    "        vif_data[\"feature\"] = X_selected.columns\n",
    "        vif_data[\"VIF\"] = [variance_inflation_factor(X_selected.values, i) for i in range(X_selected.shape[1])]\n",
    "        \n",
    "        # Drop features with high VIF (threshold can be adjusted)\n",
    "        high_vif_threshold = 5\n",
    "        features_to_drop = vif_data[vif_data[\"VIF\"] > high_vif_threshold][\"feature\"]\n",
    "        selected_features = [f for f in selected_features if f not in features_to_drop]\n",
    "        \n",
    "        # Print VIF data\n",
    "        print(\"\\nVariance Inflation Factor (VIF) for selected features:\")\n",
    "        print(vif_data)\n",
    "        \n",
    "        # Print selected features after VIF check\n",
    "        print(\"\\nSelected features after VIF check:\")\n",
    "        print(selected_features)\n",
    "        \n",
    "        return X[selected_features], selected_features\n",
    "        \n",
    "    def _build_model(self, input_dim, hidden_dims, dropout_rate):\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, h_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(dropout_rate))\n",
    "            prev_dim = h_dim\n",
    "        layers.append(nn.Linear(prev_dim, 1))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def preprocess_data(self, df, target_column, drop_columns=None):\n",
    "        if drop_columns is not None:\n",
    "            df = df.drop(columns=drop_columns)\n",
    "            \n",
    "        # Fill missing values\n",
    "        for column in df.columns:\n",
    "            if df[column].dtype == 'object':\n",
    "                df[column].fillna(df[column].mode()[0])\n",
    "                df[column] = LabelEncoder().fit_transform(df[column])\n",
    "            else:\n",
    "                df[column].fillna(df[column].median())\n",
    "        \n",
    "        X = df.drop(columns=[target_column])\n",
    "        y = df[target_column]\n",
    "        \n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Feature selection and correlation analysis\n",
    "        X_train, selected_features = self.feature_selection(X_train, y_train)\n",
    "        X_test = X_test[selected_features]\n",
    "        \n",
    "        # Update the model's input dimension based on selected features\n",
    "        self.model = self._build_model(len(selected_features), hidden_dims=self.hidden_dims, dropout_rate=self.dropout_rate).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Convert the data into tensor\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        self.train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                                     torch.tensor(y_train.values, dtype=torch.float32).view(-1,1)),\n",
    "                                       batch_size=self.batch_size, shuffle=True)\n",
    "        \n",
    "        self.test_data = (torch.tensor(X_test, dtype=torch.float32).to(self.device),\n",
    "                          torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1).to(self.device))\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test, selected_features\n",
    "        \n",
    "    def train(self):\n",
    "        best_loss = float('inf')\n",
    "        patience, trials = 10, 0\n",
    "        \n",
    "        for epoch in range(self.epochs):\n",
    "            running_loss = 0.0\n",
    "            self.model.train()\n",
    "            for inputs, labels in self.train_loader:\n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "            avg_loss = running_loss / len(self.train_loader)\n",
    "            self.train_losses.append(avg_loss)\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if avg_loss < best_loss:\n",
    "                best_loss = avg_loss\n",
    "                trials = 0\n",
    "            else:\n",
    "                trials += 1\n",
    "                if trials >= patience:\n",
    "                    print('Early stopping!')\n",
    "                    break\n",
    "        print('Finished Training')\n",
    "        return trials, epoch, avg_loss\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            inputs, y_test = self.test_data\n",
    "            outputs = self.model(inputs)\n",
    "            \n",
    "            # Convert to CPU tensors for sklearn\n",
    "            y_test = y_test.cpu().numpy()\n",
    "            predicted = outputs.cpu().numpy()\n",
    "            \n",
    "            # Report\n",
    "            mse = mean_squared_error(y_test, predicted)\n",
    "            print(f'Mean Squared Error: {mse:.2f}')\n",
    "            \n",
    "            mae = mean_absolute_error(y_test, predicted)\n",
    "            print(f'Mean Absolute Error: {mae:.2f}')\n",
    "                \n",
    "            r2 = r2_score(y_test, predicted)\n",
    "            print(f'R^2 Score: {r2:.2f}')\n",
    "            \n",
    "            # Plotting\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            # Loss curve\n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(self.train_losses, label='Training Loss')\n",
    "            plt.xlabel('Epoch')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Loss Curve')\n",
    "            plt.legend()\n",
    "            \n",
    "            # Predictions vs Actual\n",
    "            plt.subplot(1, 2, 2)\n",
    "            plt.scatter(y_test, predicted, alpha=0.7)\n",
    "            plt.xlabel('Actual Values')\n",
    "            plt.ylabel('Predicted Values')\n",
    "            plt.title('Predicted vs Actual Values')\n",
    "            plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "        return mse, mae, r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Usage Example\n",
    "\n",
    "We will demonstrate how to use the `TabularNNModel` class with different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with the California Housing dataset\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    data_choice = 3  # Choose the dataset here: 1 for California, 2 for Energy Efficiency, 3 for Concrete Compressive Strength\n",
    "    \n",
    "    # California dataset\n",
    "    if data_choice == 1:\n",
    "        from sklearn.datasets import fetch_california_housing\n",
    "        california = fetch_california_housing()\n",
    "        df = pd.DataFrame(california.data, columns=california.feature_names)\n",
    "        df['MedHousVal'] = california.target\n",
    "            \n",
    "        target_column = 'MedHousVal'\n",
    "        drop_columns = []\n",
    "\n",
    "    # Energy Efficiency dataset\n",
    "    elif data_choice == 2:\n",
    "        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx'\n",
    "        df = pd.read_excel(url)\n",
    "        \n",
    "        df.columns = ['Relative_Compactness', 'Surface_Area', 'Wall_Area', 'Roof_Area', \n",
    "                    'Overall_Height', 'Orientation', 'Glazing_Area', \n",
    "                    'Glazing_Area_Distribution', 'Heating_Load', 'Cooling_Load']\n",
    "            \n",
    "        target_column = 'Heating_Load'  # You can also use 'Cooling_Load'\n",
    "        drop_columns = []\n",
    "        \n",
    "    elif data_choice == 3:\n",
    "        # Load the Concrete Compressive Strength dataset\n",
    "        url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/concrete/compressive/Concrete_Data.xls'\n",
    "        df = pd.read_excel(url)\n",
    "\n",
    "        # Rename columns for easier reference\n",
    "        df.columns = ['Cement', 'Blast_Furnace_Slag', 'Fly_Ash', 'Water', \n",
    "                    'Superplasticizer', 'Coarse_Aggregate', 'Fine_Aggregate', \n",
    "                    'Age', 'Concrete_Compressive_Strength']\n",
    "            \n",
    "        target_column = 'Concrete_Compressive_Strength'\n",
    "        drop_columns = []\n",
    "    \n",
    "    \n",
    "    # Train and test the model    \n",
    "    model = TabularNNModel(\n",
    "                 hidden_dims=[128, 64, 32], \n",
    "                 lr=0.0001, \n",
    "                 batch_size=16, \n",
    "                 epochs=500, \n",
    "                 dropout_rate=0.0\n",
    "                 )\n",
    "    X_train, X_test, y_train, y_test, selected_features = model.preprocess_data(df, target_column, drop_columns)\n",
    "    trials, epoch, avg_loss = model.train()\n",
    "    mse, mae, r2 = model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
